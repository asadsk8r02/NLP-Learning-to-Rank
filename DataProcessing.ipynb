{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07fb8198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acd919ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features():\n",
    "    '''This function processes and creates our feature columns descriptions'''\n",
    "    # Read in the features file\n",
    "    features = pd.read_csv('features.csv')\n",
    "    # Create new header and replace spaces with underscore\n",
    "    new_header = features.iloc[0].str.replace(' ','_')\n",
    "    # Remove the first row which is now the new header\n",
    "    features = features[1:]\n",
    "    # Set new headers\n",
    "    features.columns = new_header\n",
    "    # Only the first cell for each category is filled. Using forward will\n",
    "    # will allow me to map each category to their sub-categories located\n",
    "    # in the stream column \n",
    "    features['feature_description'] = features['feature_description'].ffill()\n",
    "    # Replacing characters to allign with TensorFlows regex requirements\n",
    "    character_removal = [' ', '(', ')', '*']\n",
    "    for char in character_removal:\n",
    "        features['feature_description'] = features['feature_description'].str.replace(char, '_')\n",
    "        features['stream'] = features['stream'].astype(str).str.replace(char, '_')\n",
    "    # Setting column type to string for mapping within the load_rename_save function\n",
    "    features['feature_id'] = features['feature_id'].astype(str)\n",
    "    # Creating new column to map features to existing dataset\n",
    "    features['cols'] = 'string'\n",
    "    # Looping over all features and creating new column name\n",
    "    for idx in range(len(features)):\n",
    "        if str(features.iloc[idx]['stream']) != 'nan':\n",
    "            features['cols'].iloc[idx] = features['feature_description'].iloc[idx] + '_' + features['stream'].iloc[idx]\n",
    "        else:\n",
    "            features['cols'].iloc[idx] = features['feature_description'].iloc[idx]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddcd34b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_columns(df):\n",
    "    \n",
    "    '''This function labels the columns by descriptions\n",
    "       found on the microsoft research page'''    \n",
    "        \n",
    "    for col in df.columns:\n",
    "        if col == 0:\n",
    "            df.rename({col : 'relevance_label'}, axis=1, inplace=True)\n",
    "        elif col == 1:\n",
    "            df.rename({col : 'query_id'}, axis=1, inplace=True)\n",
    "        else:\n",
    "            df.rename({col : f'feature_{col - 1}'}, axis=1, inplace=True)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e94f7d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rename_save(folder_num):\n",
    "    '''This function reads in all data located in folder n,\n",
    "       labels the columns, removes uneeded elements from the cells (i.e. 'qid:1' the qid is uneeded),\n",
    "       and saves the files as a parquet within folder n'''\n",
    "    \n",
    "    for folder in folder_num:\n",
    "        # Load data\n",
    "        df_train = pd.read_csv(f'MSLR-WEB10K/Fold{folder}/train.txt', sep=' ', header=None)\n",
    "        df_test = pd.read_csv(f'MSLR-WEB10K/Fold{folder}/test.txt', sep=' ', header=None)\n",
    "        df_val = pd.read_csv(f'MSLR-WEB10K/Fold{folder}/vali.txt', sep=' ', header=None)\n",
    "        \n",
    "        # Label the columns\n",
    "        df_train = label_columns(df_train)\n",
    "        df_test = label_columns(df_test)\n",
    "        df_val = label_columns(df_val)\n",
    "        \n",
    "        # Remove 'n:' from each column. The dataset assigned each feature number\n",
    "        # to the cells value which needs to be removed to get the data into int/float format\n",
    "        dataframes = {'train': df_train, 'test': df_test, 'val': df_val}\n",
    "        for k, df in dataframes.items():\n",
    "            for i in range(1,len(df.columns)-1):\n",
    "                df[f'feature_{i}'].replace(f'{i}:', '', regex=True, inplace=True)          \n",
    "            \n",
    "        # Only query_id was different than all of the other columns when assigning \n",
    "        # the prefix to the values. Here we remove 'qid:' from each cell\n",
    "            df['query_id'].replace('qid:', '', regex=True, inplace=True)\n",
    "\n",
    "        # Rename the feature columns from the given descriptions on Microsofts webiste   \n",
    "        features = preprocess_features()\n",
    "        \n",
    "        for k, df in dataframes.items():\n",
    "            for idx in range(len(features)):\n",
    "                id_ = features.iloc[idx]['feature_id']\n",
    "                for col in df.columns:\n",
    "                    if str(id_) == col.lstrip('feature_'):\n",
    "                        df.rename({col: features.iloc[idx]['cols']}, axis=1, inplace=True)\n",
    "        \n",
    "        # Save the cleaned dataset as a csv\n",
    "        df_train.to_csv(f'MSLR-WEB10K/Fold{folder}/df_train.csv', index=False)\n",
    "        df_test.to_csv(f'MSLR-WEB10K/Fold{folder}/df_test.csv', index=False)\n",
    "        df_val.to_csv(f'MSLR-WEB10K/Fold{folder}/df_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c5e005e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_674543/1492805841.py:18: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  features['feature_description'] = features['feature_description'].str.replace(char, '_')\n",
      "/tmp/ipykernel_674543/1492805841.py:19: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  features['stream'] = features['stream'].astype(str).str.replace(char, '_')\n"
     ]
    }
   ],
   "source": [
    "load_rename_save([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2712fbbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevance_label</th>\n",
       "      <th>query_id</th>\n",
       "      <th>covered_query_term_number_body</th>\n",
       "      <th>covered_query_term_number_anchor</th>\n",
       "      <th>covered_query_term_number_title</th>\n",
       "      <th>covered_query_term_number_url</th>\n",
       "      <th>covered_query_term_number_whole_document</th>\n",
       "      <th>covered_query_term_ratio_body</th>\n",
       "      <th>covered_query_term_ratio_anchor</th>\n",
       "      <th>covered_query_term_ratio_title</th>\n",
       "      <th>...</th>\n",
       "      <th>Inlink_number</th>\n",
       "      <th>Outlink_number</th>\n",
       "      <th>PageRank</th>\n",
       "      <th>SiteRank</th>\n",
       "      <th>QualityScore</th>\n",
       "      <th>QualityScore2</th>\n",
       "      <th>Query-url_click_count</th>\n",
       "      <th>url_click_count</th>\n",
       "      <th>url_dwell_time</th>\n",
       "      <th>feature_137</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>11089534</td>\n",
       "      <td>2</td>\n",
       "      <td>116</td>\n",
       "      <td>64034</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>11089534</td>\n",
       "      <td>2</td>\n",
       "      <td>124</td>\n",
       "      <td>64034</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>3344</td>\n",
       "      <td>14</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>11089534</td>\n",
       "      <td>13</td>\n",
       "      <td>123</td>\n",
       "      <td>63933</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>256</td>\n",
       "      <td>49697</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   relevance_label  query_id  covered_query_term_number_body  \\\n",
       "0                2         1                               3   \n",
       "1                2         1                               3   \n",
       "2                0         1                               3   \n",
       "3                2         1                               3   \n",
       "4                1         1                               3   \n",
       "\n",
       "   covered_query_term_number_anchor  covered_query_term_number_title  \\\n",
       "0                                 3                                0   \n",
       "1                                 0                                3   \n",
       "2                                 0                                2   \n",
       "3                                 0                                3   \n",
       "4                                 0                                3   \n",
       "\n",
       "   covered_query_term_number_url  covered_query_term_number_whole_document  \\\n",
       "0                              0                                         3   \n",
       "1                              0                                         3   \n",
       "2                              0                                         3   \n",
       "3                              0                                         3   \n",
       "4                              0                                         3   \n",
       "\n",
       "   covered_query_term_ratio_body  covered_query_term_ratio_anchor  \\\n",
       "0                            1.0                              1.0   \n",
       "1                            1.0                              0.0   \n",
       "2                            1.0                              0.0   \n",
       "3                            1.0                              0.0   \n",
       "4                            1.0                              0.0   \n",
       "\n",
       "   covered_query_term_ratio_title  ...  Inlink_number  Outlink_number  \\\n",
       "0                        0.000000  ...       11089534               2   \n",
       "1                        1.000000  ...       11089534               2   \n",
       "2                        0.666667  ...              3               1   \n",
       "3                        1.000000  ...       11089534              13   \n",
       "4                        1.000000  ...              5               7   \n",
       "\n",
       "   PageRank  SiteRank  QualityScore  QualityScore2  Query-url_click_count  \\\n",
       "0       116     64034            13              3                      0   \n",
       "1       124     64034             1              2                      0   \n",
       "2       124      3344            14             67                      0   \n",
       "3       123     63933             1              3                      0   \n",
       "4       256     49697             1             13                      0   \n",
       "\n",
       "   url_click_count  url_dwell_time  feature_137  \n",
       "0                0             0.0          NaN  \n",
       "1                0             0.0          NaN  \n",
       "2                0             0.0          NaN  \n",
       "3                0             0.0          NaN  \n",
       "4                0             0.0          NaN  \n",
       "\n",
       "[5 rows x 139 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(f'MSLR-WEB10K/Fold1/df_train.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e752f6",
   "metadata": {},
   "source": [
    "###  Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17e4fcb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 83\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Train the gradient boosting LTR algorithm on the training set\u001b[39;00m\n\u001b[1;32m     82\u001b[0m ltr \u001b[38;5;241m=\u001b[39m GradientBoostingLTR(num_trees\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m---> 83\u001b[0m \u001b[43mltr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Evaluate the LTR algorithm on the test set using NDCG@10\u001b[39;00m\n\u001b[1;32m     86\u001b[0m predictions \u001b[38;5;241m=\u001b[39m ltr\u001b[38;5;241m.\u001b[39mpredict(test_features, test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n",
      "Cell \u001b[0;32mIn [27], line 50\u001b[0m, in \u001b[0;36mGradientBoostingLTR.fit\u001b[0;34m(self, X, y, qid)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_trees):\n\u001b[1;32m     49\u001b[0m     tree \u001b[38;5;241m=\u001b[39m DecisionTreeRegressor(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth)\n\u001b[0;32m---> 50\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mpredict(Xq)\n\u001b[1;32m     53\u001b[0m     gradient \u001b[38;5;241m=\u001b[39m yq \u001b[38;5;241m-\u001b[39m predictions\n",
      "File \u001b[0;32m~/anaconda3/envs/kuproj/lib/python3.9/site-packages/sklearn/tree/_classes.py:1315\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[1;32m   1279\u001b[0m     \u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, X_idx_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1280\u001b[0m ):\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m \n\u001b[1;32m   1283\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1315\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_idx_sorted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_idx_sorted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/kuproj/lib/python3.9/site-packages/sklearn/tree/_classes.py:165\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    163\u001b[0m check_X_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(dtype\u001b[38;5;241m=\u001b[39mDTYPE, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    164\u001b[0m check_y_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 165\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_separately\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_y_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[1;32m    169\u001b[0m     X\u001b[38;5;241m.\u001b[39msort_indices()\n",
      "File \u001b[0;32m~/anaconda3/envs/kuproj/lib/python3.9/site-packages/sklearn/base.py:578\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate_separately:\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# We need this because some estimators validate X and y\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;66;03m# separately, and in general, separately calling check_array()\u001b[39;00m\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;66;03m# on X and y isn't equivalent to just calling check_X_y()\u001b[39;00m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;66;03m# :(\u001b[39;00m\n\u001b[1;32m    577\u001b[0m     check_X_params, check_y_params \u001b[38;5;241m=\u001b[39m validate_separately\n\u001b[0;32m--> 578\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m     y \u001b[38;5;241m=\u001b[39m check_array(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/kuproj/lib/python3.9/site-packages/sklearn/utils/validation.py:800\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    795\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    796\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    797\u001b[0m         )\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 800\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    803\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/anaconda3/envs/kuproj/lib/python3.9/site-packages/sklearn/utils/validation.py:114\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    108\u001b[0m         allow_nan\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misinf(X)\u001b[38;5;241m.\u001b[39many()\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(X)\u001b[38;5;241m.\u001b[39mall()\n\u001b[1;32m    112\u001b[0m     ):\n\u001b[1;32m    113\u001b[0m         type_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfinity\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m allow_nan \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN, infinity\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    115\u001b[0m             msg_err\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    116\u001b[0m                 type_err, msg_dtype \u001b[38;5;28;01mif\u001b[39;00m msg_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    117\u001b[0m             )\n\u001b[1;32m    118\u001b[0m         )\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan:\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Load the MSLR-WEB10K dataset\n",
    "data = pd.read_csv(\"MSLR-WEB10K/Fold1/df_train.csv\")\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract the features and labels from the data\n",
    "train_features = train_data.iloc[:, 2:].values\n",
    "train_labels = train_data.iloc[:, 1].values\n",
    "\n",
    "val_features = val_data.iloc[:, 2:].values\n",
    "val_labels = val_data.iloc[:, 1].values\n",
    "\n",
    "test_features = test_data.iloc[:, 2:].values\n",
    "test_labels = test_data.iloc[:, 1].values\n",
    "\n",
    "# Define the evaluation metric as NDCG@10\n",
    "def ndcg(y_true, y_pred, k=10):\n",
    "    score = ndcg_score(np.array([y_true]), np.array([y_pred]), k=k)\n",
    "    return score\n",
    "\n",
    "# Define the gradient boosting algorithm for LTR\n",
    "class GradientBoostingLTR():\n",
    "    def __init__(self, num_trees=100, learning_rate=0.1, max_depth=6):\n",
    "        self.num_trees = num_trees\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def fit(self, X, y, qid):\n",
    "        self.trees = []\n",
    "        unique_qid = np.unique(qid)\n",
    "        \n",
    "        for q in unique_qid:\n",
    "            mask = qid == q\n",
    "            Xq = X[mask]\n",
    "            yq = y[mask]\n",
    "            n = len(yq)\n",
    "            weights = np.ones(n) / n\n",
    "            \n",
    "            for i in range(self.num_trees):\n",
    "                tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "                tree.fit(Xq, yq, sample_weight=weights)\n",
    "                \n",
    "                predictions = tree.predict(Xq)\n",
    "                gradient = yq - predictions\n",
    "                \n",
    "                weights = weights * np.exp(-self.learning_rate * gradient)\n",
    "                weights = weights / np.sum(weights)\n",
    "                \n",
    "                self.trees.append(tree)\n",
    "                \n",
    "    def predict(self, X, qid):\n",
    "        predictions = np.zeros(len(X))\n",
    "        unique_qid = np.unique(qid)\n",
    "        \n",
    "        for q in unique_qid:\n",
    "            mask = qid == q\n",
    "            Xq = X[mask]\n",
    "            n = len(Xq)\n",
    "            \n",
    "            if n == 0:\n",
    "                continue\n",
    "                \n",
    "            tree_predictions = np.zeros(n)\n",
    "            \n",
    "            for tree in self.trees:\n",
    "                tree_predictions += self.learning_rate * tree.predict(Xq)\n",
    "                \n",
    "            predictions[mask] = tree_predictions\n",
    "            \n",
    "        return predictions\n",
    "\n",
    "# Train the gradient boosting LTR algorithm on the training set\n",
    "ltr = GradientBoostingLTR(num_trees=100, learning_rate=0.1, max_depth=6)\n",
    "ltr.fit(train_features, train_labels, train_data['query_id'].values)\n",
    "\n",
    "# Evaluate the LTR algorithm on the test set using NDCG@10\n",
    "predictions = ltr.predict(test_features, test_data['query_id'].values)\n",
    "test_ndcg = ndcg(test_labels, predictions, k=10)\n",
    "\n",
    "print(\"Test NDCG@10:\", test_ndcg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6878dc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 77\u001b[0m\n\u001b[1;32m     74\u001b[0m     W \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m grad \u001b[38;5;241m/\u001b[39m batch_size\n\u001b[1;32m     76\u001b[0m val_preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(val_features, W)\n\u001b[0;32m---> 77\u001b[0m val_ndcg \u001b[38;5;241m=\u001b[39m \u001b[43mndcg_at_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn [28], line 29\u001b[0m, in \u001b[0;36mndcg_at_k\u001b[0;34m(preds, labels, k)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mndcg_at_k\u001b[39m(preds, labels, k):\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mndcg_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kuproj/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:1616\u001b[0m, in \u001b[0;36mndcg_score\u001b[0;34m(y_true, y_score, k, sample_weight, ignore_ties)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;124;03m\"\"\"Compute Normalized Discounted Cumulative Gain.\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \n\u001b[1;32m   1524\u001b[0m \u001b[38;5;124;03mSum the true scores ranked in the order induced by the predicted scores,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1613\u001b[0m \n\u001b[1;32m   1614\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1615\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1616\u001b[0m y_score \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1617\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[1;32m   1618\u001b[0m _check_dcg_target_type(y_true)\n",
      "File \u001b[0;32m~/anaconda3/envs/kuproj/lib/python3.9/site-packages/sklearn/utils/validation.py:800\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    795\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    796\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    797\u001b[0m         )\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 800\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    803\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/anaconda3/envs/kuproj/lib/python3.9/site-packages/sklearn/utils/validation.py:114\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    108\u001b[0m         allow_nan\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misinf(X)\u001b[38;5;241m.\u001b[39many()\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(X)\u001b[38;5;241m.\u001b[39mall()\n\u001b[1;32m    112\u001b[0m     ):\n\u001b[1;32m    113\u001b[0m         type_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfinity\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m allow_nan \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN, infinity\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    115\u001b[0m             msg_err\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    116\u001b[0m                 type_err, msg_dtype \u001b[38;5;28;01mif\u001b[39;00m msg_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    117\u001b[0m             )\n\u001b[1;32m    118\u001b[0m         )\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan:\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Load the MSLR-WEB10K dataset\n",
    "data = pd.read_csv(\"MSLR-WEB10K/Fold1/df_train.csv\")\n",
    "\n",
    "# Define the necessary functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def ranknet_loss(preds, labels):\n",
    "    n = preds.shape[0]\n",
    "    S = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if labels[i] > labels[j]:\n",
    "                S[i][j] = 1\n",
    "            elif labels[i] < labels[j]:\n",
    "                S[i][j] = -1\n",
    "    \n",
    "    P = sigmoid(np.subtract.outer(preds, preds))\n",
    "    P_diff = np.subtract.outer(P, P)\n",
    "    S_diff = np.subtract.outer(S, S)\n",
    "    return -np.sum(S_diff * P_diff) / (n * (n - 1))\n",
    "\n",
    "def ndcg_at_k(preds, labels, k):\n",
    "    return ndcg_score(np.array([labels]), np.array([preds]), k=k)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract the features and labels from the data\n",
    "train_features = train_data.iloc[:, 2:].values\n",
    "train_labels = train_data.iloc[:, 1].values\n",
    "\n",
    "val_features = val_data.iloc[:, 2:].values\n",
    "val_labels = val_data.iloc[:, 1].values\n",
    "\n",
    "test_features = test_data.iloc[:, 2:].values\n",
    "test_labels = test_data.iloc[:, 1].values\n",
    "\n",
    "# Normalize the features\n",
    "train_features = (train_features - np.mean(train_features, axis=0)) / np.std(train_features, axis=0)\n",
    "val_features = (val_features - np.mean(train_features, axis=0)) / np.std(train_features, axis=0)\n",
    "test_features = (test_features - np.mean(train_features, axis=0)) / np.std(train_features, axis=0)\n",
    "\n",
    "# Define the hyperparameters for the LTR algorithm\n",
    "num_features = train_features.shape[1]\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "# Train the LTR algorithm using mini-batch gradient descent\n",
    "W = np.zeros((num_features,))\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(train_features), batch_size):\n",
    "        batch_features = train_features[i:i+batch_size]\n",
    "        batch_labels = train_labels[i:i+batch_size]\n",
    "        \n",
    "        batch_preds = np.dot(batch_features, W)\n",
    "        batch_loss = ranknet_loss(batch_preds, batch_labels)\n",
    "        \n",
    "        grad = np.zeros((num_features,))\n",
    "        for j in range(batch_size):\n",
    "            for k in range(batch_size):\n",
    "                if batch_labels[j] > batch_labels[k]:\n",
    "                    grad += (sigmoid(batch_preds[j] - batch_preds[k]) * (batch_features[j] - batch_features[k]))\n",
    "                elif batch_labels[j] < batch_labels[k]:\n",
    "                    grad += (sigmoid(batch_preds[k] - batch_preds[j]) * (batch_features[k] - batch_features[j]))\n",
    "        \n",
    "        W -= learning_rate * grad / batch_size\n",
    "        \n",
    "    val_preds = np.dot(val_features, W)\n",
    "    val_ndcg = ndcg_at_k(val_preds, val_labels, k=10)\n",
    "    \n",
    "    print(f\"Epoch {epoch}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196fdd51",
   "metadata": {},
   "source": [
    "### Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e517d0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shailesh/anaconda3/envs/kuproj/lib/python3.9/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/shailesh/anaconda3/envs/kuproj/lib/python3.9/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "[LightGBM] [Fatal] Ranking tasks require query information\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "Ranking tasks require query information",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [32], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mDataset(train_features, label\u001b[38;5;241m=\u001b[39mtrain_labels)\n\u001b[1;32m     32\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mDataset(val_features, label\u001b[38;5;241m=\u001b[39mval_labels, reference\u001b[38;5;241m=\u001b[39mtrain_dataset)\n\u001b[0;32m---> 33\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndcg_scorer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set using ndcg_score\u001b[39;00m\n\u001b[1;32m     37\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_features)\n",
      "File \u001b[0;32m~/anaconda3/envs/kuproj/lib/python3.9/site-packages/lightgbm/engine.py:271\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# construct booster\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m     booster \u001b[38;5;241m=\u001b[39m \u001b[43mBooster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[1;32m    273\u001b[0m         booster\u001b[38;5;241m.\u001b[39mset_train_data_name(train_data_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/kuproj/lib/python3.9/site-packages/lightgbm/basic.py:2610\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[0;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[1;32m   2608\u001b[0m params_str \u001b[38;5;241m=\u001b[39m param_dict_to_str(params)\n\u001b[1;32m   2609\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_void_p()\n\u001b[0;32m-> 2610\u001b[0m \u001b[43m_safe_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterCreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_str\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2614\u001b[0m \u001b[38;5;66;03m# save reference to data\u001b[39;00m\n\u001b[1;32m   2615\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_set \u001b[38;5;241m=\u001b[39m train_set\n",
      "File \u001b[0;32m~/anaconda3/envs/kuproj/lib/python3.9/site-packages/lightgbm/basic.py:125\u001b[0m, in \u001b[0;36m_safe_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m\"\"\"Check the return value from C API call.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m    The return value from C API calls.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(_LIB\u001b[38;5;241m.\u001b[39mLGBM_GetLastError()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mLightGBMError\u001b[0m: Ranking tasks require query information"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score, make_scorer\n",
    "\n",
    "# Load the MSLR-WEB10K dataset\n",
    "data = pd.read_csv(\"MSLR-WEB10K/Fold1/df_train.csv\")\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract the features and labels from the data\n",
    "train_features = train_data.iloc[:, 2:]\n",
    "train_labels = train_data.iloc[:, 1]\n",
    "\n",
    "val_features = val_data.iloc[:, 2:]\n",
    "val_labels = val_data.iloc[:, 1]\n",
    "\n",
    "test_features = test_data.iloc[:, 2:]\n",
    "test_labels = test_data.iloc[:, 1]\n",
    "\n",
    "# Define a custom NDCG scorer\n",
    "ndcg_scorer = make_scorer(ndcg_score, needs_proba=True, k=10)\n",
    "\n",
    "# Train a LightGBM model on the training set\n",
    "params = {'objective': 'lambdarank', 'metric': 'ndcg', 'ndcg_eval_at': 10, 'learning_rate': 0.1,\n",
    "          'max_depth': 6, 'num_leaves': 64, 'verbose': 1}\n",
    "train_dataset = lgb.Dataset(train_features, label=train_labels)\n",
    "val_dataset = lgb.Dataset(val_features, label=val_labels, reference=train_dataset)\n",
    "model = lgb.train(params, train_dataset, num_boost_round=200, valid_sets=[train_dataset, val_dataset],\n",
    "                  early_stopping_rounds=10, verbose_eval=10, feval=ndcg_scorer)\n",
    "\n",
    "# Evaluate the model on the test set using ndcg_score\n",
    "predictions = model.predict(test_features)\n",
    "test_ndcg = ndcg_score(test_labels, predictions, k=10)\n",
    "\n",
    "print(\"Test NDCG@10:\", test_ndcg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc1882ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/kuproj/lib/python3.9/site-packages/pandas/core/indexes/base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/kuproj/lib/python3.9/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/kuproj/lib/python3.9/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2263\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2273\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [34], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m xi \u001b[38;5;241m=\u001b[39m train_features\u001b[38;5;241m.\u001b[39miloc[i]\n\u001b[1;32m     42\u001b[0m yi \u001b[38;5;241m=\u001b[39m train_labels\u001b[38;5;241m.\u001b[39miloc[i]\n\u001b[0;32m---> 43\u001b[0m diff_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mpairwise_diff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m s_i \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(xi, weights)\n\u001b[1;32m     45\u001b[0m gradient \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum([((diff_matrix[i][j] \u001b[38;5;241m-\u001b[39m diff_matrix[j][i]) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(s_i \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(xi, weights) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(train_features\u001b[38;5;241m.\u001b[39miloc[j], weights)))) \u001b[38;5;241m*\u001b[39m (xi \u001b[38;5;241m-\u001b[39m train_features\u001b[38;5;241m.\u001b[39miloc[j]) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_labels))], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn [34], line 30\u001b[0m, in \u001b[0;36mpairwise_diff\u001b[0;34m(labels)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[0;32m---> 30\u001b[0m         diff_matrix[i][j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m (\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m labels[j]))\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m diff_matrix\n",
      "File \u001b[0;32m~/anaconda3/envs/kuproj/lib/python3.9/site-packages/pandas/core/series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/kuproj/lib/python3.9/site-packages/pandas/core/series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_values_for_loc(\u001b[38;5;28mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m~/anaconda3/envs/kuproj/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Load the MSLR-WEB10K dataset\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract the features and labels from the data\n",
    "train_features = train_data.iloc[:, 2:]\n",
    "train_labels = train_data.iloc[:, 1]\n",
    "\n",
    "val_features = val_data.iloc[:, 2:]\n",
    "val_labels = val_data.iloc[:, 1]\n",
    "\n",
    "test_features = test_data.iloc[:, 2:]\n",
    "test_labels = test_data.iloc[:, 1]\n",
    "\n",
    "# Define a function to compute pairwise differences between labels\n",
    "def pairwise_diff(labels):\n",
    "    n = len(labels)\n",
    "    diff_matrix = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            diff_matrix[i][j] = max(0, 1 - (labels[i] - labels[j]))\n",
    "    return diff_matrix\n",
    "\n",
    "# Train a Learning to Rank model on the training set\n",
    "n_features = train_features.shape[1]\n",
    "weights = np.ones(n_features)\n",
    "eta = 0.01\n",
    "n_iterations = 100\n",
    "for iteration in range(n_iterations):\n",
    "    gradient = np.zeros(n_features)\n",
    "    for i in range(len(train_labels)):\n",
    "        xi = train_features.iloc[i]\n",
    "        yi = train_labels.iloc[i]\n",
    "        diff_matrix = pairwise_diff(train_labels)\n",
    "        s_i = np.dot(xi, weights)\n",
    "        gradient += np.sum([((diff_matrix[i][j] - diff_matrix[j][i]) / (1 + np.exp(s_i - np.dot(xi, weights) + np.dot(train_features.iloc[j], weights)))) * (xi - train_features.iloc[j]) for j in range(len(train_labels))], axis=0)\n",
    "    weights -= eta * gradient\n",
    "\n",
    "# Evaluate the model on the test set using ndcg_score\n",
    "predictions = np.dot(test_features, weights)\n",
    "test_ndcg = ndcg_score([test_labels], [predictions], k=10)\n",
    "\n",
    "print(\"Test NDCG@10:\", test_ndcg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203d5963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
